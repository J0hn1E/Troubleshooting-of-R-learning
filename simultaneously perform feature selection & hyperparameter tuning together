library(mlr3)
library(mlr3viz)
library(mlr3verse)
library(mlr3learners)
library(mlr3extralearners)
library(survival)
library(survminer)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(ggplot2)
library(gridExtra)
library(mlr3tuning)
library(mlr3fselect)

set.seed(123)
#减少记录器的冗长性
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
##读取数据
Data <- read.csv(file = "C:/Users/Desktop/A/rm_batch/data.par.csv",
                 row.names = 1,check.names = FALSE)
##数据预处理
#（1）这一步使得列名符合R变量命名规则，从而可用于mlr3
colnames(Data) <- make.names(colnames(Data))
#去掉PFS的数据
DATA_pre <- Data[,-1]


#（2）连续变量的标准化与特征选择
po_scale = po("scale")
#po("filter", filter = flt("correlation"),filter.frac = 0.7)%>>% 
po_scale$param_set$values$affect_columns =
  selector_name(colnames(DATA_pre)[-1])
#（3）数据预处理任务
task.process=as_task_classif(DATA_pre,target ="Group",positive = "1" )

DATA=po_scale$train(list(task.process))[[1]]$data()
## 真任务
task= as_task_classif(DATA, target = "Group", positive = "1")
#算法
  logreg = lrn("classif.log_reg", predict_type = "prob",
               predict_sets = c("train", "test"))
  knn = lrn("classif.kknn", scale = FALSE,
            predict_type = "prob",
            predict_sets = c("train", "test"))
  #对lda存在变量共线的问题
  lda = lrn("classif.lda", predict_type = "prob",
            predict_sets = c("train", "test"))
  #对qda来说分组太小了
  #qda = lrn("classif.qda", predict_type = "prob"),
  nb = lrn("classif.naive_bayes", predict_type = "prob",
           predict_sets = c("train", "test"))
  rpart = lrn("classif.rpart",predict_type = "prob",
              predict_sets = c("train", "test"))
  rf = lrn("classif.ranger", predict_type = "prob",
           predict_sets = c("train", "test"))
  xgb = lrn("classif.xgboost",predict_type="prob",
            predict_sets = c("train", "test"))
  svm = lrn("classif.svm", predict_type="prob",
            predict_sets = c("train", "test"))
#设置搜索空间
tune_ps_knn = ps(
    k = p_int(lower = 3, upper = 50), # Number of neighbors considered
    distance = p_dbl(lower = 1, upper = 3),
    kernel = p_fct(levels = c("rectangular", "gaussian", "rank", "optimal"))
) 
tune_ps_rpart = ps(
  # Minimum number of observations that must exist in a node in order for a
  # split to be attempted
  minsplit = p_int(lower = 10, upper = 40),
  cp = p_dbl(lower = 0.001, upper = 0.1) # Complexity parameter
)
tune_ps_rf = ps(
  # Minimum size of terminal nodes
  min.node.size = p_int(lower = 10, upper = 50),
  # Number of variables randomly sampled as candidates at each split
  mtry = p_int(lower = 1, upper = 6),
  # Number of trees
  ntree = p_int(lower=1,upper=1000)
)
tune_ps_xgb =ps(
  eta = p_dbl(lower=0,upper=1),
  gamma = p_dbl(lower=1,upper=5),
  max_depth = p_int(lower = 1, upper = 10),
  min_child_weight = p_dbl(lower = 1, upper = 10),
  subsample = p_dbl(lower = 0.5, upper = 1),
  colsample_bytree = p_dbl(lower = 0.5, upper = 1),
  nrounds = p_int(lower = 1, upper = 50)
)
tun_ps_svm = ps(
  cost = p_dbl(lower=0.1,upper=10), 
  kernel = p_fct(c("polynomial", "radial", "sigmoid")),
  degree = p_int(1, 3, depends = kernel == "polynomial"), # 设置参数依赖
  gamma = p_dbl(lower=0.1,upper=10),
  type = p_fct("C-classification")
)
at_knn = auto_tuner(
  tuner = tnr("random_search"),
  learner = knn,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_knn,
  term_evals = 100,
)
at_rpart= auto_tuner(
  tuner = tnr("random_search"),
  learner = rpart,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_rpart,
  term_evals = 100,
)
at_rf= auto_tuner(
  tuner = tnr("random_search"),
  learner = rf,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_rf,
  term_evals = 100,
)
at_xgb = auto_tuner(
  tuner = tnr("random_search"),
  learner = xgb,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_xgb,
  term_evals = 1000,
)
at_svm = auto_tuner(
  tuner = tnr("random_search"),
  learner = svm,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_svm,
  term_evals = 1000,
)
afs_logreg = auto_fselector(
  fselector = fs("random_search"),
  learner = logreg,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_knn = auto_fselector(
  fselector = fs("random_search"),
  learner = at_knn,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_lda = auto_fselector(
  fselector = fs("random_search"),
  learner = lda,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_nb = auto_fselector(
  fselector = fs("random_search"),
  learner = nb,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_rpart = auto_fselector(
  fselector = fs("random_search"),
  learner = at_rpart,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_rf = auto_fselector(
  fselector = fs("random_search"),
  learner = at_rf,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_xgb = auto_fselector(
  fselector = fs("random_search"),
  learner = at_xgb,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)
afs_svm = auto_fselector(
  fselector = fs("random_search"),
  learner = at_svm,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 1000
)

# 5-fold cross-validation
resampling_outer = rsmp(id = "cv", .key = "cv", folds = 5L)
# 自动调参过程(部分)
rr_logreg = resample(task, afs_logreg, resampling_outer, store_models = TRUE)
rr_lda = resample(task, afs_lda, resampling_outer, store_models = TRUE)
rr_nb = resample(task, afs_nb, resampling_outer, store_models = TRUE)
rr_rpart=resample(task, afs_rpart, resampling_outer, store_models = TRUE)
rr_rf = resample(task, afs_rf, resampling_outer, store_models = TRUE)
rr_xgb = resample(task, afs_xgb, resampling_outer, store_models = TRUE)  
rr_svm = resample(task, afs_svm, resampling_outer, store_models = TRUE)
#learner$param_set$values = at$tuning_result$learner_param_vals[[1]]
logreg_features <- extract_inner_fselect_results(rr_logreg)
lda_features <- extract_inner_fselect_results(rr_lda)
nb_features <- extract_inner_fselect_results(rr_nb)
rpart_features <- extract_inner_fselect_results(rr_rpart)
rf_features <- extract_inner_fselect_results(rr_rf)
xgb_features <- extract_inner_fselect_results(rr_xgb)
svm_features <- extract_inner_fselect_results(rr_svm)
#耗时较久，非常消耗算力

# Stratification
task$col_roles$stratum = task$target_names
design = benchmark_grid(
  tasks = task,
  learners = c(),#?
  resamplings = resampling_outer
)

bmr = benchmark(design, store_models = FALSE) 
measures = list(
  msr("classif.auc", predict_sets = "train", id = "auc_train"),
  msr("classif.auc", id = "auc_test")
)
