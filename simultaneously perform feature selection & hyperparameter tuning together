library(mlr3)
library(mlr3viz)
library(mlr3verse)
library(mlr3learners)
library(mlr3extralearners)
library(survival)
library(survminer)
library(dplyr)
library(tidyr)
library(DataExplorer)
library(ggplot2)
library(gridExtra)
library(mlr3tuning)
library(mlr3fselect)
library(kknn)
set.seed(123)
#减少记录器的冗长性
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
##读取数据
Data <- read.csv(file = "C:/Users/钟艺/Desktop/A/rm_batch/data.par.csv",
                 row.names = 1,check.names = FALSE)
##数据预处理
#（1）这一步使得列名符合R变量命名规则，从而可用于mlr3
colnames(Data) <- make.names(colnames(Data))
#去掉PFS的数据
DATA_pre <- Data[,-1]

#（2）连续变量的标准化
po_scale = po("scale")
po_scale$param_set$values$affect_columns =
  selector_name(colnames(DATA_pre)[-1])
#缩放任务
task_process= as_task_classif(DATA_pre, target = "Group", positive = "1")
#已缩放数据
DATA= po_scale$train(list(task_process))[[1]]$data()
#最终任务
task= as_task_classif(DATA, target = "Group", positive = "1")

#算法
pre_logreg = lrn("classif.log_reg", predict_type = "prob",
             predict_sets = c("train", "test"))
pre_knn = lrn("classif.kknn", scale = FALSE,
          predict_type = "prob",
          predict_sets = c("train", "test"))
pre_lda = lrn("classif.lda", predict_type = "prob",
          predict_sets = c("train", "test"))
pre_nb = lrn("classif.naive_bayes", predict_type = "prob",
         predict_sets = c("train", "test"))
pre_rpart = lrn("classif.rpart",predict_type = "prob",
            predict_sets = c("train", "test"))
pre_rf = lrn("classif.ranger", predict_type = "prob",
         predict_sets = c("train", "test"))
pre_xgb = lrn("classif.xgboost",predict_type="prob",
          predict_sets = c("train", "test"))
pre_svm = lrn("classif.svm", predict_type="prob",
          predict_sets = c("train", "test"))

# 设置参数空间
tune_ps_knn = ps(
  k = p_int(lower = 3, upper = 20), # Number of neighbors considered
  distance = p_dbl(lower = 1, upper = 3),
  kernel = p_fct(levels = c("rectangular", "gaussian", "rank", "optimal"))
)
tune_ps_rpart = ps(
  # Minimum number of observations that must exist in a node in order for a
  # split to be attempted
  minsplit = p_int(lower = 10, upper = 40),
  cp = p_dbl(lower = 0.001, upper = 0.1) # Complexity parameter
)
tune_ps_rf = ps(
  # Minimum size of terminal nodes
  min.node.size = p_int(lower = 10, upper = 50),
  # Number of variables randomly sampled as candidates at each split
  mtry = p_int(lower = 1, upper = 6),
  # Number of trees
  num.trees = p_int(lower=1,upper=1000)
)
tune_ps_xgb =ps(
  eta = p_dbl(lower=0,upper=1),
  gamma = p_dbl(lower=1,upper=5),
  max_depth = p_int(lower = 1, upper = 10),
  min_child_weight = p_dbl(lower = 1, upper = 10),
  subsample = p_dbl(lower = 0.5, upper = 1),
  colsample_bytree = p_dbl(lower = 0.5, upper = 1),
  nrounds = p_int(lower = 1, upper = 50)
)
tune_ps_svm = ps(
  cost = p_dbl(lower=0.1,upper=10), 
  kernel = p_fct(c("polynomial", "radial", "sigmoid")),
  degree = p_int(1, 3, depends = kernel == "polynomial"), # 设置参数依赖
  gamma = p_dbl(lower=0.1,upper=10),
  type = p_fct("C-classification")
)
#自动超参
at_knn = auto_tuner(
  tuner = tnr("random_search"),
  learner = pre_knn,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_knn,
  term_evals = 10,
)
at_rpart= auto_tuner(
  tuner = tnr("random_search"),
  learner = pre_rpart,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_rpart,
  term_evals = 10,
)
at_rf= auto_tuner(
  tuner = tnr("random_search"),
  learner = pre_rf,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_rf,
  term_evals = 10,
)
at_xgb = auto_tuner(
  tuner = tnr("random_search"),
  learner = pre_xgb,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_xgb,
  term_evals = 10,
)
at_svm = auto_tuner(
  tuner = tnr("random_search"),
  learner = pre_svm,
  resampling = rsmp("cv", folds = 5L),
  measure = msr("classif.auc"),
  search_space = tune_ps_svm,
  term_evals = 10,
)
# 3-fold cross-validation for the inner loop of the nested resampling
#嵌套重抽样概念：https://zhuanlan.zhihu.com/p/590363796
#这里是内层(对非测试集)3折分层CV重抽样，生成不同的内层训练集与验证集
#使用内层数据调参与筛选特征，下列代码迭代了term_eval次
logreg = auto_fselector(
  fselector = fs("random_search"),
  learner = pre_logreg,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10 #之后加到1000
)
knn= auto_fselector(
  fselector = fs("random_search"),
  learner = at_knn,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10 #之后加到1000
)
lda = auto_fselector(
  fselector = fs("random_search"),
  learner = pre_lda,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10 #之后加到1000
)
nb = auto_fselector(
  fselector = fs("random_search"),
  learner = pre_lda,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10 #之后加到1000
)
rpart = auto_fselector(
  fselector = fs("random_search"),
  learner = at_rpart,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10
)
rf = auto_fselector(
  fselector = fs("random_search"),
  learner = at_rf,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10
)
xgb = auto_fselector(
  fselector = fs("random_search"),
  learner = at_xgb,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10
)
svm = auto_fselector(
  fselector = fs("random_search"),
  learner = at_svm,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.auc"),
  term_evals = 10
)
# 5-fold cross-validation for the outer resampling method
resampling_outer = rsmp(id = "cv", .key = "cv", folds = 5L)
# Stratification分层采样
task$col_roles$stratum = task$target_names

logreg$predict_sets = c("train", "test")
knn$predict_sets = c("train", "test")
lda$predict_sets = c("train", "test")
nb$predict_sets = c("train", "test")
rpart$predict_sets = c("train", "test")
rf$predict_sets = c("train", "test")
xgb$predict_sets = c("train", "test")
svm$predict_sets = c("train", "test")
# 开启外层分层CV，从而引发内层分层CV
# 外层抽样的5折每一折都以内层获得的超参数与特征拟合学习器，获得在外层测试集上的性能
# 最后获得的是一份无偏的性能统计
rr_logreg = resample(task, logreg, resampling_outer, store_models = TRUE)
rr_knn = resample(task, knn, resampling_outer, store_models = TRUE)
rr_lda=resample(task, lda, resampling_outer, store_models = TRUE)
rr_nb=resample(task, nb, resampling_outer, store_models = TRUE)
rr_rpart=resample(task, rpart, resampling_outer, store_models = TRUE)
rr_rf=resample(task, rf, resampling_outer, store_models = TRUE)
rr_xgb=resample(task, xgb, resampling_outer, store_models = TRUE)
rr_svm=resample(task, svm, resampling_outer, store_models = TRUE)
#当然，到这里可以用benchmark直接比较各个模型

#提取特征
logreg_features <- extract_inner_fselect_results(rr_logreg)
logreg_f_features <- logreg_features$features#[which.max(logreg_features$classif.auc)]
knn_features <- extract_inner_fselect_results(rr_knn)
knn_f_features <- knn_features$features
lda_features <- extract_inner_fselect_results(rr_lda)
lda_f_features <- lda_features$features
nb_features <-  extract_inner_fselect_results(rr_nb)
nb_f_features <- nb_features$features
rpart_features <- extract_inner_fselect_results(rr_rpart)
rpart_f_features <- rpart_features$features
rf_features <- extract_inner_fselect_results(rr_rf)
rf_f_features <- rf_features$features
xgb_features <- extract_inner_fselect_results(rr_xgb)
xgb_f_features <- xgb_features$features
svm_features <- extract_inner_fselect_results(rr_svm)
svm_f_features <- svm_features$features

#
#精细搜索超参数

#提取超参数
knn_hyperparameters <- extract_inner_tuning_results(rr_knn)
knn_hyperparameters
#得到模型的平均表现
design = benchmark_grid(
  tasks = task,
  learners = list(logreg, knn, lda, nb, rpart, xgb, svm),
  resamplings = resampling_outer
)
bmr = benchmark(design, store_models = FALSE) 
measures = list(
  msr("classif.auc", predict_sets = "train", id = "auc_train"),
  msr("classif.auc", id = "auc_test")
)
tab = bmr$aggregate(measures)
tab_1 = tab[,c('learner_id','auc_train','auc_test')]
tab_1
